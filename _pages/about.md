---
permalink: /
title: "Seoyoon Chae"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD student working on efficient and reliable deep learning systems.  
My research focuses on making large-scale AI models more memory-efficient, deployable, and practical for real-world systems.

My current interests include:
- Model compression (quantization & sparsity)
- Memory-efficient inference for large language models (LLMs)
- KV-cache optimization for long-context decoding
- Hardware-aware algorithm design (FPGA / ASIC accelerators)
- Efficient dataflow and PIM architectures

---

## Research Vision

Modern AI models are becoming increasingly large and computationally demanding.  
I am interested in bridging the gap between algorithm design and hardware constraints — designing methods that are not only theoretically sound, but also efficient in memory, latency, and energy.

My long-term goal is to build scalable AI systems that are both intelligent and efficient.

---

## Current Research

### Memory-Efficient LLM Inference
- KV-cache quantization and adaptive precision scheduling  
- Tiling-based memory management strategies  
- Long-context reasoning efficiency analysis  

### Hardware-Aware Deep Learning
- FPGA-based DNN accelerators  
- Pipelined LayerNorm and Processing Elements array design  
- Dataflow optimization for CNN and transformer models  

---
## News

- Oct 2025 — I presented a paper at the 22nd International SoC Design Conference (ISOCC). [paper](https://chae-sy.github.io/publications/2025-isocc-llm-fpga)
- Jun 2025 — I've attended the VLSI Symposium 2025.
- Mar 2025 — I've awarded the Extra-Curricular Challenge Scholarship.


---

## Education

**[Your University Name]**  
PhD in [Your Department] (Year – Present)

**[Sungkyunkwan University]**  
B.S. in Electronic and Electrical Engineering (2022 - 2026)

---

## Contact

Email: sychaeee@g.skku.edu  
GitHub: https://github.com/chae-sy
